{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries and Packages\n",
    "\n",
    "Let's go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get the data\n",
    "#!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/concrete_data_week3.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!unzip concrete_data_week3.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Global Constants \n",
    "\n",
    "Here we will define constants that we will be using throughout the rest of the lab.\n",
    "\n",
    "   1. We are obviously dealing with two classes, so num_classes is 2.\n",
    "   2. The ResNet50 model was built and trainined using images of size (224 x 224). Therefore we will have to resize out images from (227 x 227) to (224 x 224).\n",
    "   3. We will be training and validation the model using batches of 100 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "image_resize = 224\n",
    "\n",
    "batch_size_training = 100\n",
    "batch_size_validation = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct ImageDataGenerator Instances\n",
    "\n",
    "In order to instantiate an ImageDataGenerator instance, we will set the **preprocessing_function** argument to *preprocess_input* which we import from **tensorflow.keras.applications.resnet50** in order to preprocess our images the same way the images used to train ResNet50 model were processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will use the `flow_from_directory` method to get the training images as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = data_generator.flow_from_directory (\n",
    "    'concrete_data_week3/train',\n",
    "    target_size = (image_resize , image_resize),\n",
    "    batch_size=batch_size_training,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_generator = data_generator.flow_from_directory (\n",
    "    'concrete_data_week3/valid' ,\n",
    "    target_size = (image_resize , image_resize),\n",
    "    batch_size=batch_size_training,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build, Compile and Fit Model\n",
    "\n",
    "In this section we will start building our model. We will use the Sequential model from Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will add the ResNet50 pre-trained model to out model. However, note that we don't want to include the top layer or the outpur layer of the pre-trained model. We actually want to define our own output layer and train it so that it is optimized for our image dataset. In order to leave out the output layer of the pre-trained model, we will use the argument `include_top` and set it to **False**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/pasberry/anaconda3/envs/directml/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "model.add(ResNet50 (\n",
    "    include_top=False,\n",
    "    pooling='avg' ,\n",
    "    weights='imagenet'\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will define our output layer as a **Dense** layer that consists of two nodes and uses the **Softmax** function as the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(num_classes, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the model's layers using the *layers* attribute of our model object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.engine.training.Model at 0x7f680ba4fd68>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7f6824db1748>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that our model is comprised of two sets of layers. The first set is the layers pertainig to ResNet50 and the second set is a single layer, which is our Dense layer that we defined above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the ResNet50 layers by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.engine.input_layer.InputLayer at 0x7f6824db1eb8>,\n",
       " <tensorflow.python.keras.layers.convolutional.ZeroPadding2D at 0x7f6824d5c198>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f6824d5c470>,\n",
       " <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x7f6824d5ceb8>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x7f68147a5358>,\n",
       " <tensorflow.python.keras.layers.convolutional.ZeroPadding2D at 0x7f6824d5cef0>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x7f681475c3c8>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f6814775c18>,\n",
       " <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x7f681471f6a0>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x7f68147364a8>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f681472c048>,\n",
       " <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x7f68146e1be0>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x7f68146f50b8>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f681475c1d0>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f68146e1da0>,\n",
       " <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x7f68147687f0>,\n",
       " <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x7f68146a3860>,\n",
       " <tensorflow.python.keras.layers.merge.Add at 0x7f68146afc88>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x7f68146a3630>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f68146afda0>,\n",
       " <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x7f6814662748>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x7f6814670c18>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f6814662780>,\n",
       " <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x7f6814624390>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x7f6814630828>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f68146243c8>,\n",
       " <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x7f68145d7da0>,\n",
       " <tensorflow.python.keras.layers.merge.Add at 0x7f68145ef438>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x7f68145d7c18>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f68145ef470>,\n",
       " <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x7f681459ceb8>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x7f68145b23c8>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f681459cef0>,\n",
       " <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x7f681455bc50>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x7f6814568f98>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f681456f0f0>,\n",
       " <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x7f681451c3c8>,\n",
       " <tensorflow.python.keras.layers.merge.Add at 0x7f681452bba8>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x7f681451c390>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f68144e9b38>,\n",
       " <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x7f6814490ef0>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x7f68144aa390>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f6814490550>,\n",
       " <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x7f6814457cc0>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x7f6814461f60>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f681452bcc0>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f68144690b8>,\n",
       " <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x7f68144dc6a0>,\n",
       " <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x7f68144165f8>,\n",
       " <tensorflow.python.keras.layers.merge.Add at 0x7f6814422b70>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x7f68144163c8>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f68143d0b38>,\n",
       " <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x7f68143d6668>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x7f68143e4b00>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f68143d66a0>,\n",
       " <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x7f6814399278>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x7f68143a4710>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f68143992b0>,\n",
       " <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x7f6814348c88>,\n",
       " <tensorflow.python.keras.layers.merge.Add at 0x7f6814364320>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x7f6814348160>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f6814364438>,\n",
       " <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x7f6814310dd8>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x7f68143222b0>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f6814310e10>,\n",
       " <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x7f6824ea4400>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x7f683ebc6f60>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f683ebfaf60>,\n",
       " <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x7f681427c4a8>,\n",
       " <tensorflow.python.keras.layers.merge.Add at 0x7f6814288a90>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x7f681427c438>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f68142b7ba8>,\n",
       " <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x7f681423e550>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x7f681424ca20>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f681423e588>,\n",
       " <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x7f6814200198>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x7f681420a630>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f68142001d0>,\n",
       " <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x7f6814232be0>,\n",
       " <tensorflow.python.keras.layers.merge.Add at 0x7f68141cc240>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x7f6814232f60>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f681418a4e0>,\n",
       " <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x7f681413f828>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x7f6814149978>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f681413f588>,\n",
       " <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x7f68140fd0f0>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x7f681410a588>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f68141f1ef0>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f68140fd1d0>,\n",
       " <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x7f68141f8cc0>,\n",
       " <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x7f6814137e48>,\n",
       " <tensorflow.python.keras.layers.merge.Add at 0x7f68140ca4a8>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x7f6814137cf8>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f68140ca4e0>,\n",
       " <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x7f681407fba8>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x7f68140900b8>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f681407fbe0>,\n",
       " <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x7f6814040940>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x7f681404bc88>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f68140408d0>,\n",
       " <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x7f68140023c8>,\n",
       " <tensorflow.python.keras.layers.merge.Add at 0x7f681400cba8>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x7f6814002400>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f680bfc04a8>,\n",
       " <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x7f680bfc6320>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x7f680bfd37b8>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f680bfc6358>,\n",
       " <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x7f680bf79320>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x7f680bf923c8>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f680bf79390>,\n",
       " <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x7f680bf3fc50>,\n",
       " <tensorflow.python.keras.layers.merge.Add at 0x7f680bf502e8>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x7f680bf3f9b0>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f680bf50400>,\n",
       " <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x7f680bf08a20>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x7f680bf15eb8>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f680bf08a58>,\n",
       " <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x7f680bec8748>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x7f680bed0ac8>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f680bec8550>,\n",
       " <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x7f680be89160>,\n",
       " <tensorflow.python.keras.layers.merge.Add at 0x7f680be949e8>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x7f680be89128>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f680be94a20>,\n",
       " <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x7f680be4f128>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x7f680be595f8>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f680be4f160>,\n",
       " <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x7f680be07780>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x7f680be1c208>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f680be07f60>,\n",
       " <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x7f680bdc75c0>,\n",
       " <tensorflow.python.keras.layers.merge.Add at 0x7f680bddc128>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x7f680bdc7b70>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f680bddc240>,\n",
       " <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x7f680bd90860>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x7f680bd9acf8>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f680bd90898>,\n",
       " <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x7f680bd4e470>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x7f680bd5b908>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f680bd4e5c0>,\n",
       " <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x7f680bd0f080>,\n",
       " <tensorflow.python.keras.layers.merge.Add at 0x7f680bd1d828>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x7f680bd0f0b8>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f680bce2748>,\n",
       " <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x7f680bc96978>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x7f680bca3be0>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f680bc96940>,\n",
       " <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x7f680bc56320>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x7f680bc617f0>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f680bd1d860>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f680bc56358>,\n",
       " <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x7f680bccff28>,\n",
       " <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x7f680bc082b0>,\n",
       " <tensorflow.python.keras.layers.merge.Add at 0x7f680bc24710>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x7f680bc08278>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f680bc24748>,\n",
       " <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x7f680bbd3e48>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x7f680bbe8320>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f680bbd3e80>,\n",
       " <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x7f680bb96860>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x7f680bba1ef0>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f680bbaa048>,\n",
       " <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x7f680bb587b8>,\n",
       " <tensorflow.python.keras.layers.merge.Add at 0x7f680bb64e10>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x7f680bb58748>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f680bb17e48>,\n",
       " <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x7f680bb1e588>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x7f680bb2ca20>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f680bb1e5c0>,\n",
       " <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x7f680badd198>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x7f680baec630>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f680badd1d0>,\n",
       " <tensorflow.python.keras.layers.normalization.BatchNormalization at 0x7f680ba98c18>,\n",
       " <tensorflow.python.keras.layers.merge.Add at 0x7f680baa9518>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x7f680ba98e10>,\n",
       " <tensorflow.python.keras.layers.pooling.GlobalAveragePooling2D at 0x7f680ba57a90>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the ResNet50 model has already been trained, then we want to tell our model not to bother with training the ResNet part, but to train only our dense out layer. To do that, we run the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now using the summary attribute of the model we can see how many parameters we will need to optimize in order to train the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Model)             (None, 2048)              23587712  \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 4098      \n",
      "=================================================================\n",
      "Total params: 23,591,810\n",
      "Trainable params: 4,098\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we compile our model using the **adam** optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam' , loss='categorical_crossentropy', metrics = ['accuracy', 'mse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we are able to start the training process, with an ImageDataGenerator, we will need to define how many steps compose an epoch. Typically that is the number of images divided by the batch size. Therefor, we define out steps per epoch as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch_training = len(train_generator)\n",
    "steps_per_epoch_validation = len(validation_generator)\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to start training the model. Unlik a conventional deep learning trainig were data is not streamed from a directory, with an ImageDataGenerator where data is augmented in batches, we use the `fit_gnerator` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "299/300 [============================>.] - ETA: 2s - loss: 0.0379 - acc: 0.9887 - mean_squared_error: 0.0094Epoch 1/2\n",
      "300/300 [==============================] - 1095s 4s/step - loss: 0.0379 - acc: 0.9887 - mean_squared_error: 0.0094 - val_loss: 0.0886 - val_acc: 0.9671 - val_mean_squared_error: 0.0256\n",
      "Epoch 2/2\n",
      "299/300 [============================>.] - ETA: 3s - loss: 0.0097 - acc: 0.9975 - mean_squared_error: 0.0022Epoch 1/2\n",
      "300/300 [==============================] - 1186s 4s/step - loss: 0.0097 - acc: 0.9975 - mean_squared_error: 0.0022 - val_loss: 0.0727 - val_acc: 0.9738 - val_mean_squared_error: 0.0202\n"
     ]
    }
   ],
   "source": [
    "fit_history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch_training,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=steps_per_epoch_validation,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained, you are ready to start using it to classify images. \n",
    "\n",
    "Since training can take a long time when building deep learning models, it is always a good idea to save your model once the training is complete if you believe you will be using the model again later. You will be using this model in the next module so go ahead and save your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('classifier_resnet_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
